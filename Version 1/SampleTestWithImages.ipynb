{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6106f5-8db1-4373-b1bc-052a392d9766",
   "metadata": {},
   "source": [
    "# Batch Gender Detection and Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34c799-08ea-4ea1-8fde-ffdb089cadf9",
   "metadata": {},
   "source": [
    "Processes a batch of images in a specified folder, performs gender detection, and saves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23d46e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import cvlib as cv\n",
    "import os\n",
    "\n",
    "def process_images_in_folder(source_folder, destination_folder):\n",
    "    # Load the gender detection model\n",
    "    model = load_model('gender_detection_largeDataset_v2.model')\n",
    "\n",
    "    # Define the classes for gender labels\n",
    "    classes = ['man', 'woman']\n",
    "\n",
    "    # Iterate through all files in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            input_image_path = os.path.join(source_folder, filename)\n",
    "\n",
    "            # Read the input image\n",
    "            image = cv2.imread(input_image_path)\n",
    "\n",
    "            # Apply face detection using cvlib\n",
    "            faces, confidences = cv.detect_face(image)\n",
    "\n",
    "            # Loop through detected faces\n",
    "            for idx, face in enumerate(faces):\n",
    "                # Get corner points of face rectangle\n",
    "                (startX, startY) = face[0], face[1]\n",
    "                (endX, endY) = face[2], face[3]\n",
    "\n",
    "                # Crop the detected face region from the image\n",
    "                face_crop = np.copy(image[startY:endY, startX:endX])\n",
    "\n",
    "                # If the cropped face region is too small, skip further processing\n",
    "                if face_crop.shape[0] < 10 or face_crop.shape[1] < 10:\n",
    "                    continue\n",
    "\n",
    "                # Preprocess the cropped face for gender detection\n",
    "                face_crop = cv2.resize(face_crop, (96, 96))\n",
    "                face_crop = face_crop.astype(\"float\") / 255.0\n",
    "                face_crop = img_to_array(face_crop)\n",
    "                face_crop = np.expand_dims(face_crop, axis=0)\n",
    "\n",
    "                # Apply gender detection on face using the loaded model\n",
    "                conf = model.predict(face_crop)[0]\n",
    "\n",
    "                # Get the predicted gender label and confidence score\n",
    "                idx = np.argmax(conf)\n",
    "                label = classes[idx]\n",
    "                confidence = conf[idx] * 100\n",
    "\n",
    "                # Draw a rectangle around the detected face\n",
    "                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "                # Add text label above the face rectangle\n",
    "                label_text = f\"{label.capitalize()} ({confidence:.2f}%)\"\n",
    "                cv2.putText(image, label_text, (startX, startY - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # Save the output image to the destination folder\n",
    "            output_image_path = os.path.join(destination_folder, filename)\n",
    "            cv2.imwrite(output_image_path, image)\n",
    "\n",
    "# Example usage\n",
    "source_folder = r'.\\test_images'\n",
    "destination_folder = r'.\\output_images'\n",
    "process_images_in_folder(source_folder, destination_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5adb62-2253-4dbf-b94c-9966b6fa91a4",
   "metadata": {},
   "source": [
    "Processes a batch of images in a specified folder, performs gender detection, and dominant top wear color detection and saves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327e46d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Result saved to: .\\output_images_2\\result_-little-indian-girls-ready-brush-teeth-isolated-green-background_466689-8858.jpg\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Result saved to: .\\output_images_2\\result_IMG-20240103-WA0004.jpg\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Result saved to: .\\output_images_2\\result_IMG-20240103-WA0005.jpg\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Result saved to: .\\output_images_2\\result_IMG-20240121-WA0021.jpg\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Result saved to: .\\output_images_2\\result_IMG-20240121-WA0024.jpg\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Result saved to: .\\output_images_2\\result_IMG-20240121-WA0032.jpg\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-boys-posing-photo-village-near-hampi-india-39234734.jpg\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-boys-team-child-preview.jpg\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-cute-small-girl-christian-ceremony-39407355.jpg\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-girl-child-playing-with-ball_54391-1817.jpg\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-girl-child-smiling-face-260nw-788133373.jpg\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-girl-folded-arms-portrait-standing-thumbnail.jpg\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-girls-26533114.jpg\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-little-girl-in-traditional-dress-celebrating-diwali-or-deepawali-ai-generated-photo.jpg\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Result saved to: .\\output_images_2\\result_indian-rural-girl-sitting-front-door-36453568.jpg\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Result saved to: .\\output_images_2\\result_Indian-school-girls.jpg\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Result saved to: .\\output_images_2\\result_istockphoto-1310209956-612x612.jpg\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Result saved to: .\\output_images_2\\result_istockphoto-613557584-612x612.jpg\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Result saved to: .\\output_images_2\\result_l-kid-in-happy-and-joyful-mood-the-photo-shows-female-child-tiny-tot-smiling.jpg\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Result saved to: .\\output_images_2\\result_little-indian-boy-illustration-by-generative-ai_75474-8616.jpg\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Result saved to: .\\output_images_2\\result_mber-circa-unidentified-poor-indian-boy-smiling-serious-eyes-looks-122840649.jpg\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Result saved to: .\\output_images_2\\result_Modern-Indian-Baby-Boy-Names-With-Meanings2-910x1024.jpg\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Result saved to: .\\output_images_2\\result_ng-going-school-with-small-school-bag-isolated-white-background_466689-16743.jpg\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Result saved to: .\\output_images_2\\result_number-heavy-books-her-head-white-background-study-burden-children-220310227.jpg\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Result saved to: .\\output_images_2\\result_people-kids-kid-children.jpg\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Result saved to: .\\output_images_2\\result_pexels-photo-5884273.jpeg\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Result saved to: .\\output_images_2\\result_ph_31919_115974.jpg\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Result saved to: .\\output_images_2\\result_pict_large.jpg\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Result saved to: .\\output_images_2\\result_png-transparent-india-girl-child-ethnic-group-graphy-bollywood-toddler-infant-world.png\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Result saved to: .\\output_images_2\\result_pngtree-poor-indian-boy-child-india-kid-photo-image_22837622.jpg\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Result saved to: .\\output_images_2\\result_poor-indian-boys-8473880.jpg\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Result saved to: .\\output_images_2\\result_poor-indian-girl-child-260nw-1134293303.jpg\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Result saved to: .\\output_images_2\\result_poor-little-girl-boken-image-260nw-2225837405.jpg\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Result saved to: .\\output_images_2\\result_portrait-cute-little-indian-girl-model-sitting-isolated-white-background_466689-45022.jpg\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_1.jpg\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_10.jpg\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_11.jpg\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_2.jpg\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_4.jpg\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_5.jpg\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_7.jpg\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Result saved to: .\\output_images_2\\result_test_8.jpg\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Result saved to: .\\output_images_2\\result_WhatsApp Image 2023-11-11 at 21.50.56_b38d02ea.jpg\n"
     ]
    }
   ],
   "source": [
    "def process_images_in_folder(source_folder, destination_folder):\n",
    "    # Load the gender detection model\n",
    "    gender_model = load_model('gender_detection_largeDataset_v2.model')\n",
    "\n",
    "    # Iterate through all files in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            input_image_path = os.path.join(source_folder, filename)\n",
    "\n",
    "            # Read the input image\n",
    "            frame = cv2.imread(input_image_path)\n",
    "\n",
    "            # Apply face detection using cvlib\n",
    "            faces, confidences = cv.detect_face(frame)\n",
    "\n",
    "            for idx, face in enumerate(faces):\n",
    "                (startX, startY) = face[0], face[1]\n",
    "                (endX, endY) = face[2], face[3]\n",
    "\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "                face_crop = np.copy(frame[startY:endY, startX:endX])\n",
    "\n",
    "                # Crop a region just below the detected face (adjusting it lower and wider)\n",
    "                dress_region = frame[endY + 100:endY + 250, startX:endX]\n",
    "\n",
    "                if (face_crop.shape[0]) < 10 or (face_crop.shape[1]) < 10:\n",
    "                    continue\n",
    "\n",
    "                face_crop = cv2.resize(face_crop, (96, 96))\n",
    "                face_crop = face_crop.astype(\"float\") / 255.0\n",
    "                face_crop = img_to_array(face_crop)\n",
    "                face_crop = np.expand_dims(face_crop, axis=0)\n",
    "\n",
    "                gender_conf = gender_model.predict(face_crop)[0]\n",
    "                gender_label = \"Woman\" if gender_conf[1] > gender_conf[0] else \"Man\"\n",
    "                gender_percentage = max(gender_conf) * 100\n",
    "\n",
    "                # Calculate the color histogram in the dress region\n",
    "                hist = cv2.calcHist([dress_region], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "                dominant_color_bin = np.unravel_index(hist.argmax(), hist.shape)\n",
    "                dominant_color_bgr = [int(b * 32) for b in dominant_color_bin]\n",
    "\n",
    "                # Display the gender and dress color with percentage and color name near each face\n",
    "                gender_text = f\"Gender: {gender_label} ({gender_percentage:.2f}%)\"\n",
    "                color_text = f\"Dress Color: {dominant_color_bgr}\"\n",
    "\n",
    "                labelY = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\n",
    "                cv2.putText(frame, gender_text, (startX, labelY), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, color_text, (startX, labelY + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, dominant_color_bgr, 2)\n",
    "\n",
    "                # Draw a rectangle around the dress color region\n",
    "                cv2.rectangle(frame, (startX, endY + 100), (endX, endY + 250), dominant_color_bgr, 2)\n",
    "\n",
    "            # Save the resulting image with information to the destination folder\n",
    "            output_path = os.path.join(destination_folder, f\"result_{filename}\")\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            print(f\"Result saved to: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "source_folder = r'.\\test_images'\n",
    "destination_folder = r'.\\output_images_2'\n",
    "process_images_in_folder(source_folder, destination_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genderClassification",
   "language": "python",
   "name": "genderclassification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
